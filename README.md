# caesar225-langgraph-MAT496

In module 1 video 1, I learnt about the motivation behind LangGraph. A solitary language model alone is somewhat limited, as it doesn't have access to tools, as an example. So, many LLM applications use some kind of control flow with steps before and after LLM calls, like tool calls or retrieval steps. This control flow forms a chain. But, we want LLM systems that can pick their control flow. Here's where an agent comes in. Simply put, an agent is control flow that's defined by an LLM. We can vary the amount of control we give to the LLM applications, from low to high. On the lower end, we have routers and on the other end, we have fully autonomous agents. The relationship between control and reliability is an inverse one. 
This is where LangGraph comes in, as it helps us build agents with high reliability, even while we give them more control. We can express custom control flows as graphs. They contain nodes, which we can think about as steps in our application. Edges are just the connectivity between nodes and can route control flow based on LLM decisions. LangGraph has a lot of advanced controllability features like persistence, streaming, human in the loop, etc.

In module 1 video 2, we built a simple graph. A graph has normal edges, and conditional edges. The state is basically the object that we pass between the nodes and edges of the graph. Here, it's a dictionary with one key called graph state. Each node takes in the state and overwrites the value of graph state. Here, based on 50-50 odds, we go to node 2 or 3 and thus, we get different outputs each time. I tweaked the code with my own examples.

In module 1 video 3, I learnt how to work with LangGraph studio. This allows us to interact with graphs visually. Threads capture the history of any run of our graph.

In module 1 video 4, I learnt about chains, which combine a few core concepts, such as the idea of chat messages, chat models, binding tools and executing tool calls, all within LangGraph. Chat models interact with messages. Tools are needed when we want to connect our chat models with an external API that requires some payload to run. We use these messages as graph state. Here, we append to our state, instead of overwriting it, as we want to preserve the history in a chat model. This leads to reducer functions. These functions tell LangGraph to append to the messages list when we receive a new message. Then we run two types of inputs on our graph, one being natural language input and the other being a message that's expected to elicit a tool call. I tweaked the code, so that it uses Groq and not Open AI and also changed all the examples.

In module 1 video 5, I learnt about routers. They are nodes that route execution to different next steps depending on some logic. This is a simple kind of agent. The LLM is directing the control flow of the application. Here, we added a node that will actually call a tool, so that we can execute that tool call in a separate node. We also added a conditional edge that lets us look at the chat model output and make a decision about where it will route to. In our case, natural language responses will route to the end, and the others will route to the tool call. I tweaked the code to make it compatible with Groq, and played around with the examples. I also changed the function from multiply to divide. 

In module 1 video 6, I learnt how to expand upon the stuff in the previous video to create a more popular generic agent architecture. We added an edge from the tools node, back to the model, in a kind of loop. This loop will run as long as there will be tool calls. The model will call tools, depending on the user input. We will take the tool output and return it back to the model. Finally, the model will call tools as long as it sees fit and then it will call a natural language response and end the sequence. I modified the code to work with Groq, and tweaked the example a bit.
