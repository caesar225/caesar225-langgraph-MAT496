# caesar225-langgraph-MAT496

In module 1 video 1, I learnt about the motivation behind LangGraph. A solitary language model alone is somewhat limited, as it doesn't have access to tools, as an example. So, many LLM applications use some kind of control flow with steps before and after LLM calls, like tool calls or retrieval steps. This control flow forms a chain. But, we want LLM systems that can pick their control flow. Here's where an agent comes in. Simply put, an agent is control flow that's defined by an LLM. We can vary the amount of control we give to the LLM applications, from low to high. On the lower end, we have routers and on the other end, we have fully autonomous agents. The relationship between control and reliability is an inverse one. 
This is where LangGraph comes in, as it helps us build agents with high reliability, even while we give them more control. We can express custom control flows as graphs. They contain nodes, which we can think about as steps in our application. Edges are just the connectivity between nodes and can route control flow based on LLM decisions. LangGraph has a lot of advanced controllability features like persistence, streaming, human in the loop, etc.

In module 1 video 2, we built a simple graph. A graph has normal edges, and conditional edges. The state is basically the object that we pass between the nodes and edges of the graph. Here, it's a dictionary with one key called graph state. Each node takes in the state and overwrites the value of graph state. Here, based on 50-50 odds, we go to node 2 or 3 and thus, we get different outputs each time. I tweaked the code with my own examples.

In module 1 video 3, I learnt how to work with LangGraph studio. This allows us to interact with graphs visually. Threads capture the history of any run of our graph.

In module 1 video 4, I learnt about chains, which combine a few core concepts, such as the idea of chat messages, chat models, binding tools and executing tool calls, all within LangGraph. Chat models interact with messages. Tools are needed when we want to connect our chat models with an external API that requires some payload to run. We use these messages as graph state. Here, we append to our state, instead of overwriting it, as we want to preserve the history in a chat model. This leads to reducer functions. These functions tell LangGraph to append to the messages list when we receive a new message. Then we run two types of inputs on our graph, one being natural language input and the other being a message that's expected to elicit a tool call. I tweaked the code, so that it uses Groq and not Open AI and also changed all the examples.

In module 1 video 5, I learnt about routers. They are nodes that route execution to different next steps depending on some logic. This is a simple kind of agent. The LLM is directing the control flow of the application. Here, we added a node that will actually call a tool, so that we can execute that tool call in a separate node. We also added a conditional edge that lets us look at the chat model output and make a decision about where it will route to. In our case, natural language responses will route to the end, and the others will route to the tool call. I tweaked the code to make it compatible with Groq, and played around with the examples. I also changed the function from multiply to divide. 

In module 1 video 6, I learnt how to expand upon the stuff in the previous video to create a more popular generic agent architecture. We added an edge from the tools node, back to the model, in a kind of loop. This loop will run as long as there will be tool calls. The model will call tools, depending on the user input. We will take the tool output and return it back to the model. Finally, the model will call tools as long as it sees fit and then it will call a natural language response and end the sequence. I modified the code to work with Groq, and tweaked the example a bit.

In module 1 video 7, we extend the architecture of act, observe and reason to introduce the idea of memory. In our previous code, there is no connection between different graph states. LangChain uses check pointers to save the graph state after each step, and this gives memory. They save the state at each node, which is equivalent to a step. So, the key thing is to know this check pointer at every step in our graph. We'll write a checkpoint, which contains the state of the graph at that point. And they can be associated together in a thread, which is basically a collection of checkpoints. In the later invocations, we pass in the thread ID. I changed the code to make it compatible with Groq and played around with the numbers.


In module 2 video 1, I learnt more about state schema. It's something we have to specify when we define a LangGraph state graph, as it's the structure and the types of the data the graph will use. So far we had been using TypedDict to establish schema for our graph, but now we also use Python dataclasses. However, the problem with both of them is that we can assign an invalid value without it raising an error. Pydantic solves this issue and provides data validation. I tweaked the name value of the moods to see edge cases.

In module 2 video 2, I learnt about state reducers, which specify how state updates are performed on specific keys in our schema. By default, when we make updates, we overwrite the prior value in the node. But we can make only one update per step. Reducers help by allowing us to specify how to perform state updates. In our specific case, we are just appending the new values to the foo list. There can be exceptions to the type of updates normal reducers can handle, so we have custom reducers, for ex, for handling null inputs which are incompatible with list. I changed the numbers used in the examples to form a sequence.

In module 2 video 3, I learnt about the different ways to customize a schema. First, we have the private state, which is sort of like encapsulation as it covers the intermediate working logic. Then, we have the cases where me may have different input and output schemas, so we can restrict what is present in the output for the end user to see. I added a currency conversion graph, using the same principles as shown in the video.

In module 2 video 4, I learnt about the difficulties in managing a long running conversation. To overcome them, first we used reducers to delete all but the most recent messages. However, we may not always want to modify the graph state, so we can filter the messages we pass to the chat model. We can do this by using a filtered list. I also learnt about trimming messages, which restricts the message history(the context basically) to a specified number of tokens. I reconfigured the code to work with Groq, instead of OpenAI and I also changed all the examples used for a better understanding.

In module 2 video 5, I learnt about another way of compression that tries to preserve information better than just trimming or filtering old messages. For memory, we can compile our graph with an in memory checkpointer, which allows us to persist the memory of our conversation, as long as, in my case, the notebook session is in existence. We can set an arbitrary number for the number of messages required to start producing a running summary of the conversation, which will continue indefinitely. I reconfigured the code to work with Groq and changed the name made examples.
