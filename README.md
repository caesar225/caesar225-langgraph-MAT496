# caesar225-langgraph-MAT496

In module 1 video 1, I learnt about the motivation behind LangGraph. A solitary language model alone is somewhat limited, as it doesn't have access to tools, as an example. So, many LLM applications use some kind of control flow with steps before and after LLM calls, like tool calls or retrieval steps. This control flow forms a chain. But, we want LLM systems that can pick their control flow. Here's where an agent comes in. Simply put, an agent is control flow that's defined by an LLM. We can vary the amount of control we give to the LLM applications, from low to high. On the lower end, we have routers and on the other end, we have fully autonomous agents. The relationship between control and reliability is an inverse one. 
This is where LangGraph comes in, as it helps us build agents with high reliability, even while we give them more control. We can express custom control flows as graphs. They contain nodes, which we can think about as steps in our application. Edges are just the connectivity between nodes and can route control flow based on LLM decisions. LangGraph has a lot of advanced controllability features like persistence, streaming, human in the loop, etc.

In module 1 video 2, we built a simple graph. A graph has normal edges, and conditional edges. The state is basically the object that we pass between the nodes and edges of the graph. Here, it's a dictionary with one key called graph state. Each node takes in the state and overwrites the value of graph state. Here, based on 50-50 odds, we go to node 2 or 3 and thus, we get different outputs each time. I tweaked the code with my own examples. https://github.com/caesar225/caesar225-langgraph-MAT496/blob/main/simple-graph%20(1).ipynb

In module 1 video 3, I learnt how to work with LangGraph studio. This allows us to interact with graphs visually. Threads capture the history of any run of our graph.

In module 1 video 4, I learnt about chains, which combine a few core concepts, such as the idea of chat messages, chat models, binding tools and executing tool calls, all within LangGraph. Chat models interact with messages. Tools are needed when we want to connect our chat models with an external API that requires some payload to run. We use these messages as graph state. Here, we append to our state, instead of overwriting it, as we want to preserve the history in a chat model. This leads to reducer functions. These functions tell LangGraph to append to the messages list when we receive a new message. Then we run two types of inputs on our graph, one being natural language input and the other being a message that's expected to elicit a tool call. I tweaked the code, so that it uses Groq and not Open AI and also changed all the examples. https://github.com/caesar225/caesar225-langgraph-MAT496/blob/main/chain%20(1).ipynb

In module 1 video 5, I learnt about routers. They are nodes that route execution to different next steps depending on some logic. This is a simple kind of agent. The LLM is directing the control flow of the application. Here, we added a node that will actually call a tool, so that we can execute that tool call in a separate node. We also added a conditional edge that lets us look at the chat model output and make a decision about where it will route to. In our case, natural language responses will route to the end, and the others will route to the tool call. I tweaked the code to make it compatible with Groq, and played around with the examples. I also changed the function from multiply to divide. https://github.com/caesar225/caesar225-langgraph-MAT496/blob/main/router%20(1).ipynb

In module 1 video 6, I learnt how to expand upon the stuff in the previous video to create a more popular generic agent architecture. We added an edge from the tools node, back to the model, in a kind of loop. This loop will run as long as there will be tool calls. The model will call tools, depending on the user input. We will take the tool output and return it back to the model. Finally, the model will call tools as long as it sees fit and then it will call a natural language response and end the sequence. I modified the code to work with Groq, and tweaked the example a bit. https://github.com/caesar225/caesar225-langgraph-MAT496/blob/main/agent%20(1).ipynb

In module 1 video 7, we extend the architecture of act, observe and reason to introduce the idea of memory. In our previous code, there is no connection between different graph states. LangChain uses check pointers to save the graph state after each step, and this gives memory. They save the state at each node, which is equivalent to a step. So, the key thing is to know this check pointer at every step in our graph. We'll write a checkpoint, which contains the state of the graph at that point. And they can be associated together in a thread, which is basically a collection of checkpoints. In the later invocations, we pass in the thread ID. I changed the code to make it compatible with Groq and played around with the numbers. https://github.com/caesar225/caesar225-langgraph-MAT496/blob/main/agent-memory%20(1).ipynb


In module 2 video 1, I learnt more about state schema. It's something we have to specify when we define a LangGraph state graph, as it's the structure and the types of the data the graph will use. So far we had been using TypedDict to establish schema for our graph, but now we also use Python dataclasses. However, the problem with both of them is that we can assign an invalid value without it raising an error. Pydantic solves this issue and provides data validation. I tweaked the name value of the moods to see edge cases. https://github.com/caesar225/caesar225-langgraph-MAT496/blob/main/state-schema%20(1).ipynb

In module 2 video 2, I learnt about state reducers, which specify how state updates are performed on specific keys in our schema. By default, when we make updates, we overwrite the prior value in the node. But we can make only one update per step. Reducers help by allowing us to specify how to perform state updates. In our specific case, we are just appending the new values to the foo list. There can be exceptions to the type of updates normal reducers can handle, so we have custom reducers, for ex, for handling null inputs which are incompatible with list. I changed the numbers used in the examples to form a sequence. https://github.com/caesar225/caesar225-langgraph-MAT496/blob/main/state-reducers.ipynb

In module 2 video 3, I learnt about the different ways to customize a schema. First, we have the private state, which is sort of like encapsulation as it covers the intermediate working logic. Then, we have the cases where me may have different input and output schemas, so we can restrict what is present in the output for the end user to see. I added a currency conversion graph, using the same principles as shown in the video. https://github.com/caesar225/caesar225-langgraph-MAT496/blob/main/multiple-schemas.ipynb

In module 2 video 4, I learnt about the difficulties in managing a long running conversation. To overcome them, first we used reducers to delete all but the most recent messages. However, we may not always want to modify the graph state, so we can filter the messages we pass to the chat model. We can do this by using a filtered list. I also learnt about trimming messages, which restricts the message history(the context basically) to a specified number of tokens. I reconfigured the code to work with Groq, instead of OpenAI and I also changed all the examples used for a better understanding. https://github.com/caesar225/caesar225-langgraph-MAT496/blob/main/trim-filter-messages.ipynb

In module 2 video 5, I learnt about another way of compression that tries to preserve information better than just trimming or filtering old messages. For memory, we can compile our graph with an in memory checkpointer, which allows us to persist the memory of our conversation, as long as, in my case, the notebook session is in existence. We can set an arbitrary number for the number of messages required to start producing a running summary of the conversation, which will continue indefinitely. I reconfigured the code to work with Groq and changed the name made examples. https://github.com/caesar225/caesar225-langgraph-MAT496/blob/main/chatbot-summarization.ipynb

In module 2 video 6, I learnt about the limitation of the chatbot we built in the previous file. This chatbot can't persist indefinitely. It only lives through the lifetime of our notebook session. LangGraph solves this by supporting a few checkpointers that work with external databases, instead of an in-memory checkpointer. Here, we use Sqlite, which is a very popular SQL database. The benefit of using a checkpointer like Sqlite is that we're writing to a local database on the machine, so it's persisted over time. When we restart our session, we don't have to rerun the graph, we just have to grab the graph state by supplying the thread ID. I rewrote the code to handle a Groq model, instead of OpenAI. I also fixed a Windows shell syntax issue which occured when the given code was using commands like mkdir -p, etc. Then, I created my own chatbot which would give me a summary after 4 messages, and saw that it gave me a summary as expected. Also, after I wrote new messages, it gave me their summary aswell. https://github.com/caesar225/caesar225-langgraph-MAT496/blob/main/chatbot-external-memory.ipynb


In module 3 video 1, we built up on memory by using human in the loop which allows users to interact with agents or graphs in many ways. I also learnt about streaming, which provides several ways to visualise graph output. We have stream and astream, which are sync and async methods for streaming back results. LangGraph supports streaming modes like updates, which streams updates to the state of the graph after each node is called, and values, which streams the full state of the graph after each node is called. Since many graphs contain LLMs, which produce tokens. We often want to stream them as they're generated by the LLM. We can do this with the .astream_events method, which emits events as they occur inside the nodes. Each event is a dict with event, name, data, and metadata (which includes langgraph_node). We also learned about the LangGraph API and how it can be used directly from the IDE. In addition, we explored the new streaming method that updates the final message token by token to display the output in real time. I reconfigured the code to make it compatible with Groq, instead of OpenAI. I also created my own graph and changed all the examples. https://github.com/caesar225/caesar225-langgraph-MAT496/blob/main/streaming-interruption.ipynb

In module 3 video 2, I learnt that streaming allows us to emit the graph state at every step, which sets up human in the loop. Some use cases for human in the loop are approval, debugging or directly editing the state of the graph or agent with human feedback. Breakpoints are a way to execute human-in-the-loop as they can stop the graph at specific steps or nodes. In the case of a tool calling agent, we can use interrupt_before="tools" where tools is our tools node. This will stop the agent from calling any tools, and it will seek user approval. If the user approves, then we can use Graph.stream(None,{thread_id}), which will execute the graph from the current state, in our case, the tools node. I reconfigured the code to work with Groq, instead of OpenAI. I also used breakpoints to create an interactive calculator. https://github.com/caesar225/caesar225-langgraph-MAT496/blob/main/breakpoints.ipynb

In module 3 video 3, I learnt how to edit the graph state once it stopped through the means of breakpoints. Here, we interrupt before we actually run the assistant. We do this using the interrupt_before method. We will have to run the code once more as after using tools to find the answer, it again goes back to interrupt. If we want to explicitly get user input to modify a state, we can do that easily by supplying a dummy node which will be a no-op node. It will accept the user feedback and inject it into the graph. By using the dummy node we will add feedback as if we are running that node and we can do this by simply with "as_node" field in the update state function call. I reconfigured the code to work with Groq and added more tools and changed the state of the graph aswell. Then, I continued with the execution of the graph. https://github.com/caesar225/caesar225-langgraph-MAT496/blob/main/edit-state-human-feedback.ipynb

In module 3 video 4, I learnt that sometimes we want the graph to be able to interrupt itself, like an internal breakpoint, also called dynamic breakpoints, based on some flag or condition. This can be achieved using NodeInterrupt. If the condition is met, then we cannot proceed further in the graph unless we change the state. I created a graph which pauses on its own whenever the input message contains 'Stop'. After the interruption, I manually updated the state by changing the message to "Continue process" and then resumed the graph execution using the update_state() and graph.stream() methods. https://github.com/caesar225/caesar225-langgraph-MAT496/blob/main/dynamic-breakpoints.ipynb

In module 3 video 5, I learnt how LangGraph supports debugging by viewing, re-playing, and even forking from past states, which is called time travel. We can use get_state to look at the current state of our graph, given the thread_id. Similarly, get_state_history lets us get the state at all prior steps, so we can also browse the state history of our agent. To replay our agent from any of its prior steps, we pass checkpoint id and thread id to Graph.stream method. It is important to note that replay does not mean we re execute any action, as the graph knows that the checkpoint has been executed before. Forking is something we have already seen. When we have to update the state of the graph, we pass the thread id and the state we wanna update with. So, we are running from the same step, but with a different input. This is called forking. If we want to rewind and fork a prior checkpoint, we need to pass the checkpoint id aswell. Here, we re-execute the graph. When updating the state, we have to remember what state key we're updating and what's the reducer on that key. In our case, it's add_messages, which will append unless we supply the message id aswell. I reconfigured the code to work with Groq. I also messed around with time travel by re-running the graph from various checkpoints. I also added a new function to assess how altering the state affected the final output. I also saw how time travel can be used to debug or experiment with a wide variety of inputs. https://github.com/caesar225/caesar225-langgraph-MAT496/blob/main/time-travel.ipynb


In module 4 video 1, I learnt that fanning in and out is something we often want to in graphs. If we want to run multiple steps in the same step, then we say that we are fanning out. In our example, B and C are trying to update the state in the same step, which is ambiguous, so an error pops up. So, if we're running certain nodes in the same step, that is, in parallel, and they're writing to the same key, we need to use a reducer that can aggregate those updates. I also saw what happens when branches don't have the same number of steps. Then all the steps in the branches will be executed first and then the next step which they fan into will be executed. We can also customize the order of updates when they're within the same step using a custom reducer. We used it in a practical way by using web search and Wikipedia search in parallel to answer our questions through an LLM. This is a good practical use case for parallelization. If we want to retrieve information from multiple sources and aggregate it in a particular key, and use that later for answer generation using an LLM. I refactored the code to work with Groq. I made my own use case which can be used to summarise the history of a company from Wikipedia as well as the latest news about it. https://github.com/caesar225/caesar225-langgraph-MAT496/blob/main/parallelization.ipynb

In module 4 video 2, I learnt about another important controllability topic, which is sub graphs. They allow us to create and manage different states within different parts of our graph. This is really useful for multi agent systems, like teams of agents that have their own state. The key thing to understand was, how does the parent graph communnicate with the sub graphs in terms of state and vice versa. I learnt that this is done with overlapping keys. In the example used, we specify an output schema aswell for the sub graph as it will contain only that information which we want to service to the user at the end. I created my own version of the subgraphs and parent graph. Instead of using the 'failure analysis' and 'question summarization' subgraphs from the tutorial, I designed a research assistant. The first subgraphs performs a web search and summarizes the results in 1 line. The second subgraph extracts key insights from the results and the main graph cleans the raw input, runs both subgraphs in parallel and combines the outputs with a reducer to produce a final summary. https://github.com/caesar225/caesar225-langgraph-MAT496/blob/main/sub-graph.ipynb

In module 4 video 3, I learnt about Map reduce, which is another important controllability topic. It is basically an efficient task decomposition and parallel processing method. It has 2 phases. The first one is Map phase, that is, take some task, break it into a bunch of subtasks and do them all in parallel. The second one is Reduce, which is aggregate the results from all those parallelized subtasks and bring them back together. We then looked at a simple example in which the map phase generates a set of jokes about a topic, and the reduce phase selects the best joke from that set. I saw how Send was used in the LangGraph API which sends every statement in a list to a particular node, in our case, generate_joke. I also added a code to print the number of jokes generated. https://github.com/caesar225/caesar225-langgraph-MAT496/blob/main/map-reduce.ipynb

In module 4 video 4, we pulled together the themes of memory, Human-in-the-loop and controllability in a realistic multi agent system that's capable of open ended research. Raw LLMs are poorly suited for real-world decision-making workflows because we may want to provide specific sources for research and we may want to specify output formats that aid in high quality decision making. First, we are gonna give the system access to arbitrary sources. We're gonna take in a topic given by the user, break it into subtopics, and assign an AI analyst to each subtopic. We'll use human-in-the-loop to refine those subtopics. Then, we're going to orchestrate a dialogue between each of our analysts and an expert that has access to the sources(like an AI-AI roleplay). The analyst will ask questions and the expert will answer them. We'll pull all those conversations together in one parallelised process and use Map Reduce to bring the results together at the end. Then, we'll finally synthesise it to a final report. We pallelise the interviews using the Send() API. We also used sub graphs for separate concerns. I refactored the code to work with Groq. I also added a code to print the number of analysts in the final report. https://github.com/caesar225/caesar225-langgraph-MAT496/blob/main/research-assistant.ipynb
