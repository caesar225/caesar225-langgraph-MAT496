# caesar225-langgraph-MAT496

In module 1 video 1, I learnt about the motivation behind LangGraph. A solitary language model alone is somewhat limited, as it doesn't have access to tools, as an example. So, many LLM applications use some kind of control flow with steps before and after LLM calls, like tool calls or retrieval steps. This control flow forms a chain. But, we want LLM systems that can pick their control flow. Here's where an agent comes in. Simply put, an agent is control flow that's defined by an LLM. We can vary the amount of control we give to the LLM applications, from low to high. On the lower end, we have routers and on the other end, we have fully autonomous agents. The relationship between control and reliability is an inverse one. 
This is where LangGraph comes in, as it helps us build agents with high reliability, even while we give them more control. We can express custom control flows as graphs. They contain nodes, which we can think about as steps in our application. Edges are just the connectivity between nodes and can route control flow based on LLM decisions. LangGraph has a lot of advanced controllability features like persistence, streaming, human in the loop, etc.

In module 1 video 2, we built a simple graph. A graph has normal edges, and conditional edges. The state is basically the object that we pass between the nodes and edges of the graph. Here, it's a dictionary with one key called graph state. Each node takes in the state and overwrites the value of graph state. Here, based on 50-50 odds, we go to node 2 or 3 and thus, we get different outputs each time. I tweaked the code with my own examples.

In module 1 video 3, I learnt how to work with LangGraph studio. This allows us to interact with graphs visually. Threads capture the history of any run of our graph.

In module 1 video 4, I learnt about chains, which combine a few core concepts, such as the idea of chat messages, chat models, binding tools and executing tool calls, all within LangGraph. Chat models interact with messages. Tools are needed when we want to connect our chat models with an external API that requires some payload to run. We use these messages as graph state. Here, we append to our state, instead of overwriting it, as we want to preserve the history in a chat model. This leads to reducer functions. These functions tell LangGraph to append to the messages list when we receive a new message. Then we run two types of inputs on our graph, one being natural language input and the other being a message that's expected to elicit a tool call. I tweaked the code, so that it uses Groq and not Open AI and also changed all the examples.

In module 1 video 5, I learnt about routers. They are nodes that route execution to different next steps depending on some logic. This is a simple kind of agent. The LLM is directing the control flow of the application. Here, we added a node that will actually call a tool, so that we can execute that tool call in a separate node. We also added a conditional edge that lets us look at the chat model output and make a decision about where it will route to. In our case, natural language responses will route to the end, and the others will route to the tool call. I tweaked the code to make it compatible with Groq, and played around with the examples. I also changed the function from multiply to divide. 

In module 1 video 6, I learnt how to expand upon the stuff in the previous video to create a more popular generic agent architecture. We added an edge from the tools node, back to the model, in a kind of loop. This loop will run as long as there will be tool calls. The model will call tools, depending on the user input. We will take the tool output and return it back to the model. Finally, the model will call tools as long as it sees fit and then it will call a natural language response and end the sequence. I modified the code to work with Groq, and tweaked the example a bit.

In module 1 video 7, we extend the architecture of act, observe and reason to introduce the idea of memory. In our previous code, there is no connection between different graph states. LangChain uses check pointers to save the graph state after each step, and this gives memory. They save the state at each node, which is equivalent to a step. So, the key thing is to know this check pointer at every step in our graph. We'll write a checkpoint, which contains the state of the graph at that point. And they can be associated together in a thread, which is basically a collection of checkpoints. In the later invocations, we pass in the thread ID. I changed the code to make it compatible with Groq and played around with the numbers.


In module 2 video 1, I learnt more about state schema. It's something we have to specify when we define a LangGraph state graph, as it's the structure and the types of the data the graph will use. So far we had been using TypedDict to establish schema for our graph, but now we also use Python dataclasses. However, the problem with both of them is that we can assign an invalid value without it raising an error. Pydantic solves this issue and provides data validation. I tweaked the name value of the moods to see edge cases.

In module 2 video 2, I learnt about state reducers, which specify how state updates are performed on specific keys in our schema. By default, when we make updates, we overwrite the prior value in the node. But we can make only one update per step. Reducers help by allowing us to specify how to perform state updates. In our specific case, we are just appending the new values to the foo list. There can be exceptions to the type of updates normal reducers can handle, so we have custom reducers, for ex, for handling null inputs which are incompatible with list. I changed the numbers used in the examples to form a sequence.

In module 2 video 3, I learnt about the different ways to customize a schema. First, we have the private state, which is sort of like encapsulation as it covers the intermediate working logic. Then, we have the cases where me may have different input and output schemas, so we can restrict what is present in the output for the end user to see. I added a currency conversion graph, using the same principles as shown in the video.

In module 2 video 4, I learnt about the difficulties in managing a long running conversation. To overcome them, first we used reducers to delete all but the most recent messages. However, we may not always want to modify the graph state, so we can filter the messages we pass to the chat model. We can do this by using a filtered list. I also learnt about trimming messages, which restricts the message history(the context basically) to a specified number of tokens. I reconfigured the code to work with Groq, instead of OpenAI and I also changed all the examples used for a better understanding.

In module 2 video 5, I learnt about another way of compression that tries to preserve information better than just trimming or filtering old messages. For memory, we can compile our graph with an in memory checkpointer, which allows us to persist the memory of our conversation, as long as, in my case, the notebook session is in existence. We can set an arbitrary number for the number of messages required to start producing a running summary of the conversation, which will continue indefinitely. I reconfigured the code to work with Groq and changed the name made examples.

In module 2 video 6, I learnt about the limitation of the chatbot we built in the previous file. This chatbot can't persist indefinitely. It only lives through the lifetime of our notebook session. LangGraph solves this by supporting a few checkpointers that work with external databases, instead of an in-memory checkpointer. Here, we use Sqlite, which is a very popular SQL database. The benefit of using a checkpointer like Sqlite is that we're writing to a local database on the machine, so it's persisted over time. When we restart our session, we don't have to rerun the graph, we just have to grab the graph state by supplying the thread ID. I rewrote the code to handle a Groq model, instead of OpenAI. I also fixed a Windows shell syntax issue which occured when the given code was using commands like mkdir -p, etc. Then, I created my own chatbot which would give me a summary after 4 messages, and saw that it gave me a summary as expected. Also, after I wrote new messages, it gave me their summary aswell.


In module 3 video 1, we built up on memory by using human in the loop which allows users to interact with agents or graphs in many ways. I also learnt about streaming, which provides several ways to visualise graph output. We have stream and astream, which are sync and async methods for streaming back results. LangGraph supports streaming modes like updates, which streams updates to the state of the graph after each node is called, and values, which streams the full state of the graph after each node is called. Since many graphs contain LLMs, which produce tokens. We often want to stream them as they're generated by the LLM. We can do this with the .astream_events method, which emits events as they occur inside the nodes. Each event is a dict with event, name, data, and metadata (which includes langgraph_node). We also learned about the LangGraph API and how it can be used directly from the IDE. In addition, we explored the new streaming method that updates the final message token by token to display the output in real time. I reconfigured the code to make it compatible with Groq, instead of OpenAI. I also created my own graph and changed all the examples. https://github.com/caesar225/caesar225-langgraph-MAT496/blob/main/streaming-interruption.ipynb

In module 3 video 2, I learnt that streaming allows us to emit the graph state at every step, which sets up human in the loop. Some use cases for human in the loop are approval, debugging or directly editing the state of the graph or agent with human feedback. Breakpoints are a way to execute human-in-the-loop as they can stop the graph at specific steps or nodes. In the case of a tool calling agent, we can use interrupt_before="tools" where tools is our tools node. This will stop the agent from calling any tools, and it will seek user approval. If the user approves, then we can use Graph.stream(None,{thread_id}), which will execute the graph from the current state, in our case, the tools node. I reconfigured the code to work with Groq, instead of OpenAI. I also used breakpoints to create an interactive calculator. https://github.com/caesar225/caesar225-langgraph-MAT496/blob/main/breakpoints.ipynb

In module 3 video 3,

